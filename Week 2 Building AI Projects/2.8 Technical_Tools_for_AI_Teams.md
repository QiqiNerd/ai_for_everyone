# Week 2 – Lesson 8: Technical Tools for AI Teams

## 1. Introduction

When working with AI teams, you may hear them talk about the tools they use to build AI systems. In this video, we’ll go through the most commonly used AI tools so that you can better understand what AI engineers are working with and how you can collaborate with them.

AI development today is **open-source (开源的)**, meaning that a lot of the tools and frameworks are publicly available for anyone to use. This has made it easier for AI teams to work quickly and effectively.

## 2. Common AI Tools and Frameworks

Some of the most commonly used **AI tools (人工智能工具)** in machine learning include:

- **PyTorch (深度学习框架 PyTorch)**
- **TensorFlow (深度学习框架 TensorFlow)**
- **Hugging Face (Hugging Face)**
- **PaddlePaddle (百度的深度学习框架 PaddlePaddle)**
- **Scikit-Learn (机器学习库 Scikit-Learn)**
- **R (统计计算和图形 R)**

These are popular frameworks used to build machine learning systems and conduct experiments in AI. Many AI breakthroughs are shared freely on the **archive (arXiv)** and **GitHub (GitHub)**, where open-source projects can be found and used by anyone.

## 3. GitHub and Open-Source Projects

**GitHub (GitHub)** is a platform where many AI teams share their **code (代码)**. By using open-source software from GitHub, teams can get started faster without having to build everything from scratch. For instance, searching for **face recognition software (人脸识别软件)** on GitHub may show various projects that are freely available.

Make sure to check the **license (许可)** before using these projects in production, but open-source software is often available for everyone to use.

## 4. Hardware for AI Systems

AI systems also rely on specialized **hardware (硬件)** to process large amounts of data. Two of the most important types of hardware are:

- **CPU (中央处理单元)**: The **central processing unit (CPU)** is the main processor in most computers, including desktops, laptops, and cloud servers. It handles general computations.
- **GPU (图形处理单元)**: The **graphics processing unit (GPU)** was originally built to process graphics for gaming but has since proven highly effective for training **neural networks (神经网络)** and **deep learning (深度学习)** models. Companies like **Nvidia (英伟达)** and **Google (谷歌)** have developed specialized hardware like **TPUs (张量处理单元)** for AI workloads.

### 4.1 Why GPUs?

**GPUs (图形处理单元)** are optimized for parallel processing, which is ideal for training large neural networks. Their computational power has made them essential for **deep learning (深度学习)**.

## 5. Cloud vs. On-Premises vs. Edge Deployments

### 5.1 Cloud Deployments

**Cloud computing (云计算)** refers to using remote servers, like **Amazon’s AWS (亚马逊的AWS)**, **Microsoft’s Azure (微软的Azure)**, or **Google’s GCP (谷歌的GCP)**, to rent **compute resources (计算资源)** for AI projects. Cloud deployments allow teams to quickly scale their infrastructure without having to manage hardware themselves.

### 5.2 On-Premises Deployments

In **on-premises deployments (本地部署)**, the compute resources are physically located within the company, allowing for more control over the infrastructure. This is less common for AI but is still used by some organizations that require strict data security.

### 5.3 Edge Deployments

**Edge computing (边缘计算)** refers to processing data **locally (本地)**, near the data source, rather than sending it to a central server or the cloud. For example, in **self-driving cars (自动驾驶汽车)**, AI processes sensor data locally in the car to make decisions quickly. Similarly, **smart speakers (智能音响)** use edge computing to process speech recognition tasks locally.

The advantage of **edge computing (边缘计算)** is that it reduces **latency (延迟)** and bandwidth use by processing data directly on the device.

## 6. Summary

In this video, you learned about the common **AI tools (人工智能工具)** used by engineers, including **PyTorch (PyTorch)**, **TensorFlow (TensorFlow)**, and **GitHub (GitHub)** for sharing code. You also learned about the hardware needed for AI, such as **GPUs (GPU)**, and the three types of deployments: **cloud (云计算)**, **on-premises (本地部署)**, and **edge computing (边缘计算)**.

In the next video, we will explore how AI projects fit within the larger context of a company.