# Week 4 – Lesson 3: Discrimination / Bias

AI systems have the potential to greatly impact our lives, but one of the most critical issues in AI today is **bias** (偏见). AI can learn to discriminate against certain groups, leading to unfair or harmful outcomes. In this lesson, we will explore how AI systems become biased and discuss the methods to reduce or eliminate these biases.

## 1. How Does AI Become Biased? (AI如何变得有偏见)

AI systems learn patterns from data, and if the data reflects **societal biases** (社会偏见), the AI system can perpetuate or even amplify these biases. Let’s take a look at a notable example from **Microsoft** (微软), which showed that when AI learns from **text on the internet** (互联网上的文本), it can internalize unhealthy stereotypes.

### 1.1 The Stereotype Example (刻板印象示例)
- Microsoft researchers found that when an AI is asked to reason about analogies, such as "man is to woman as father is to what?" it outputs the word **mother** (母亲), which reflects how these words are commonly used on the internet.
- Similarly, when the analogy "man is to computer programmer as woman is to what?" was presented, the AI system answered **homemaker** (家庭主妇), a clear example of bias. 
- A more **fair answer** (公平答案) would be "woman is to computer programmer," just as "man is to computer programmer."

This bias happens because AI systems learn from the patterns in the data they are trained on, and in this case, the data reflects stereotypical associations between gender and professions.

### 1.2 Technical Details: How AI Learns Bias (AI如何学习偏见的技术细节)
- The AI stores words as **numbers** (数字), representing how these words are used across the internet. For example, the word "man" might be represented as the numbers (1,1), while "computer programmer" might be represented as (3,2), and "woman" as (2,3).
- When the AI is asked to solve analogies, such as "man is to computer programmer as woman is to what?" it calculates the relative distances between these numbers. In the case of biased data, this can result in biased outputs, like associating "woman" with "homemaker."

## 2. Real-World Examples of Bias (偏见的现实世界例子)

AI systems are already making important decisions in real life, and bias in these systems can have harmful consequences.

### 2.1 Hiring Bias (招聘偏见)
- One company used an AI system for **hiring** (招聘), but found that the system **discriminated against women** (歧视女性). The AI was trained on historical hiring data, which reflected gender bias. This led to unfair hiring decisions, and the company ultimately had to shut down the tool.
  
### 2.2 Bias in Facial Recognition (面部识别中的偏见)
- Facial recognition systems have shown to be **more accurate for lighter-skinned individuals** (浅肤色个体), but less accurate for **darker-skinned individuals** (深肤色个体). This is due to the data being primarily collected from lighter-skinned people.
- Such bias is problematic, especially when these systems are used in **criminal investigations** (刑事调查), potentially leading to unfair consequences for individuals with darker skin.

### 2.3 Discriminatory Loan Approval Systems (歧视性贷款审批系统)
- AI systems used for **loan approvals** (贷款审批) have sometimes been found to discriminate against certain **minority ethnic groups** (少数民族群体). For example, some AI systems have quoted **higher interest rates** (更高的利率) to individuals from these groups, even though they are equally creditworthy.

## 3. Addressing AI Bias (应对AI偏见)

The AI community is actively working on solutions to reduce and eliminate bias in AI systems. Here are some strategies that can help:

### 3.1 Technical Solutions for Bias Reduction (减少偏见的技术解决方案)
- **Zeroing Out Bias-Causing Numbers** (将引起偏见的数字置为零): Researchers have found that when an AI system learns from data, some specific numbers represent biases. By "zeroing out" these numbers (setting them to zero), the AI’s bias can be reduced.
  
- **Inclusive Data** (包容性数据): One way to combat bias is to use **more diverse and inclusive data** (更多元和包容性的数据). For example, a **face-recognition system** (面部识别系统) should be trained on data from multiple **ethnicities** (种族) and **genders** (性别), ensuring that the AI system is less biased and can perform well across different demographic groups.

### 3.2 Transparency and Auditing (透明度与审计)
- **Transparency** (透明度) and **auditing** (审计) processes are essential to monitor AI systems for bias. By regularly checking the performance of AI systems on various **subgroups** (子群体) of the population, we can identify if certain groups are being unfairly treated.
- For example, face recognition teams regularly check how accurate their systems are on **dark-skinned** versus **light-skinned** individuals to ensure the system doesn't show biased behavior.

### 3.3 Diverse Workforce (多样化的劳动力)
- Having a **diverse workforce** (多样化的劳动力) within AI development teams can also help identify and reduce bias. Diverse teams are more likely to notice and address biases that may be overlooked by more homogenous teams.
- A **diverse perspective** (多样化的观点) can lead to more inclusive data collection and algorithm design, ensuring that AI systems are designed to be **fair** (公平的) and **inclusive** (包容的).

## 4. The Importance of Addressing Bias (解决偏见的重要性)

AI systems are making decisions that affect people's lives, such as in hiring, credit approval, and law enforcement. Bias in AI can **reinforce harmful stereotypes** (强化有害刻板印象), contributing to societal inequalities. For example:
- If a young girl searches for images of a **Chief Executive Officer** (首席执行官) and only sees men, this could discourage her from pursuing a career in leadership, perpetuating gender inequality.

**Key Insight:** By addressing bias in AI, we can ensure that these systems promote **fairness** (公平), **equality** (平等), and **inclusivity** (包容性), rather than reinforcing existing societal inequalities.

## 5. Conclusion (结论)

Bias in AI is a serious issue that requires ongoing attention. While we are making progress in identifying and mitigating bias, it remains a significant challenge. By implementing technical solutions, using diverse and inclusive data, promoting transparency, and fostering diverse teams, we can work toward creating AI systems that are more **fair** and **equitable** for everyone.

In the next video, we will dive deeper into the issue of **adversarial attacks** (对抗性攻击) and explore strategies to protect AI systems from such threats.
