# Week 4 – Lesson 4: Adversarial Attacks on AI

Despite the incredible power of modern AI, particularly deep learning, these systems can be **fooled** (欺骗) in surprising ways. One of the limitations of AI, especially in areas like **image recognition** (图像识别), is its **susceptibility to adversarial attacks** (对抗性攻击). In this lesson, we will explore how AI can be tricked, the implications of such attacks, and the ongoing efforts to protect AI systems from being misled.

## 1. What is an Adversarial Attack? (什么是对抗性攻击？)

An **adversarial attack** (对抗性攻击) occurs when someone deliberately manipulates input data in such a way that the AI system makes an **incorrect decision** (错误的决定). These attacks are particularly dangerous because the modifications to the input data are often **imperceptible** (无法察觉) to humans, but they cause AI systems to behave in ways that would be unrecognizable to a human observer.

### 1.1 Example: Image Classification (图像分类示例)
- Suppose you give an AI system a picture of a **bird** (鸟) and ask it to classify it. The AI system outputs **hummingbird** (蜂鸟), which is a reasonable result.
- However, if you make a **minor perturbation** (微小扰动) to the image — adjusting the pixel values just a little bit — the same AI system might now classify the image as a **hammer** (锤子), even though the image looks almost identical to the human eye.
- This shows that while humans see almost no difference, the AI system perceives the image in a way that allows it to be fooled by subtle changes.

### 1.2 Real-World Examples of Adversarial Attacks (现实世界中的对抗性攻击示例)
- In one example, an AI system was shown a picture of a **hare** (野兔). After applying a small change to the pixel values, the AI mistakenly classified it as a **desk** (书桌), which is an example of how easily AI can be tricked by adversarial modifications.
  
**Key Insight:** AI systems can be tricked into **misclassifying objects** (误分类物体) by changes that humans cannot even perceive, making them vulnerable to **deliberate manipulation** (故意操控).

## 2. Types of Adversarial Attacks (对抗性攻击的类型)

### 2.1 Attacks on Digital Data (数字数据上的攻击)
Some adversarial attacks require the ability to **modify an image** (修改图像) directly. For example:
- **Spam Filters** (垃圾邮件过滤器): These systems can be vulnerable to attackers who subtly alter images in emails, allowing spam to bypass detection.
  
### 2.2 Physical World Attacks (物理世界中的攻击)
Adversarial attacks are not limited to digital data; they can also affect physical objects. Here are some examples:

#### 2.2.1 **Glasses to Fool Facial Recognition** (用眼镜欺骗面部识别)
- A team at **Carnegie Mellon University** (卡内基梅隆大学) designed a pair of funky glasses that could **fool an AI system** into thinking a person was **actress Milla Jovovich** (演员米拉·乔沃维奇). This example illustrates how something as simple as wearing specific glasses can **deceive** (欺骗) an AI system.

#### 2.2.2 **Stickers on a Stop Sign** (贴纸覆盖停车标志)
- A group of researchers from **UC Berkeley** (加州大学伯克利分校) and **University of Michigan** (密歇根大学) showed that by placing **stickers** (贴纸) on a stop sign, they could trick an AI system used in **self-driving cars** (自动驾驶汽车) into failing to recognize the stop sign altogether.
- While humans still see the sign as a stop sign, the AI system might mistake it for something else, leading to a **dangerous misinterpretation** (危险误解) in critical scenarios.

#### 2.2.3 **Sticker on a Banana** (贴纸欺骗香蕉识别)
- In another example from **Google researchers** (谷歌研究人员), a sticker was placed on a **banana** (香蕉), and the AI system misclassified the image as a **toaster** (烤面包机), showcasing how even simple changes can dramatically alter an AI's understanding of the image.
  
**Key Insight:** Physical attacks are especially dangerous in real-world applications like **self-driving cars** (自动驾驶汽车) and other systems that require accurate real-time decision-making.

## 3. Defending Against Adversarial Attacks (如何防御对抗性攻击)

Fortunately, the AI community is working on methods to make AI systems more **resistant** (抗拒) to adversarial attacks. However, these defenses are not without challenges.

### 3.1 Technical Solutions for Defense (防御的技术解决方案)
- **Neural Network Modifications** (神经网络修改): One approach to defending against adversarial attacks involves modifying neural networks to make them **less susceptible** (不易受攻击) to small changes in data.
- **Cost of Defenses** (防御的成本): Implementing these defenses can **slow down** (减慢) the AI system’s performance, as the system becomes more complex in order to detect and counter adversarial inputs.

### 3.2 AI and the Arms Race (人工智能与军备竞赛)
- Similar to the battle between **spam filters** and **spammers** (垃圾邮件发送者), there is an emerging **arms race** (军备竞赛) between AI defenders and attackers.
- AI defenders will work to build more robust defenses, while attackers will continue to find new ways to fool the systems.
  
### 3.3 Practical Considerations (实际考虑)
- Not all AI systems face adversarial attacks. For example, systems used for **quality control** in a factory (工厂质量控制) may not be targeted by adversaries, so their defenses against adversarial attacks may not need to be as strong.
- However, for **high-risk applications** (高风险应用), such as **autonomous vehicles** (自动驾驶车辆), **financial systems** (金融系统), and **security systems** (安全系统), adversarial defense is critical to ensure safety and reliability.

## 4. Conclusion (结论)

Adversarial attacks represent a significant challenge to the development of **secure AI systems** (安全的AI系统). These attacks exploit the **vulnerabilities** (脆弱性) in AI systems, often causing them to make mistakes that humans would never make. As AI continues to be deployed in **critical applications** (关键应用), defending against adversarial attacks will become an increasingly important area of research.

Although adversarial attacks are a real concern, the AI community is actively working to build more **robust** (稳健的) and **secure** (安全的) systems. As we move forward, it’s essential that we continue to improve defenses and stay vigilant in protecting AI systems from adversarial threats.

In the next video, we will explore **adverse use cases** (不良应用案例) of AI and discuss how to address them.
